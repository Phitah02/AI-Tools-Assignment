{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Iris Species Classification using Decision Tree Classifier\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Iris dataset using a Decision Tree Classifier. \n",
    "We will cover data exploration, preprocessing, model training, evaluation, and interpretation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Libraries](#Import-Libraries)\n",
    "2. [Load and Explore Dataset](#Load-and-Explore-Dataset)\n",
    "3. [Data Preprocessing](#Data-Preprocessing)\n",
    "4. [Split Data](#Split-Data)\n",
    "5. [Train Decision Tree Classifier](#Train-Decision-Tree-Classifier)\n",
    "6. [Make Predictions](#Make-Predictions)\n",
    "7. [Evaluate Model](#Evaluate-Model)\n",
    "8. [Feature Importance](#Feature-Importance)\n",
    "9. [Example Predictions](#Example-Predictions)\n",
    "10. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We start by importing all necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up matplotlib for inline plotting in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set seaborn style for better visualizations\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset\n",
    "\n",
    "In this section, we load the Iris dataset and perform initial exploration to understand its structure, content, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and explore the dataset\n",
    "print(\"Step 1: Loading and exploring the dataset...\")\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset information:\")\n",
    "print(df.info())\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing involves cleaning the data and preparing it for machine learning. This includes handling missing values, removing unnecessary columns, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "print(\"\\nStep 2: Data Preprocessing...\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop the 'Id' column as it's not useful for prediction\n",
    "df = df.drop('Id', axis=1)\n",
    "print(\"\\nDropped 'Id' column\")\n",
    "\n",
    "# Encode the target variable (Species) from categorical to numerical\n",
    "label_encoder = LabelEncoder()\n",
    "df['Species_encoded'] = label_encoder.fit_transform(df['Species'])\n",
    "print(\"\\nLabel encoding mapping:\")\n",
    "for i, species in enumerate(label_encoder.classes_):\n",
    "    print(f\"{species}: {i}\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(['Species', 'Species_encoded'], axis=1)  # Features\n",
    "y = df['Species_encoded']  # Target variable\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "We split the dataset into training and testing sets to evaluate the model's performance on unseen data. Using stratification ensures balanced class distribution in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the data into training and testing sets\n",
    "print(\"\\nStep 3: Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,  # 70% training, 30% testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Maintain class distribution in splits\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Training set class distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "print(f\"Testing set class distribution:\\n{pd.Series(y_test).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Decision Tree Classifier\n",
    "\n",
    "We initialize and train a Decision Tree Classifier. The max_depth parameter is set to prevent overfitting by limiting the tree's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Decision Tree Classifier\n",
    "print(\"\\nStep 4: Training Decision Tree Classifier...\")\n",
    "# Create decision tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    random_state=42,  # For reproducibility\n",
    "    max_depth=3  # Limit tree depth to prevent overfitting\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "print(\"Decision Tree classifier trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Using the trained model, we make predictions on the test set and display some examples to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions\n",
    "print(\"\\nStep 5: Making predictions...\")\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "y_pred_proba = dt_classifier.predict_proba(X_test)\n",
    "\n",
    "print(\"First 5 predictions:\")\n",
    "for i in range(5):\n",
    "    actual_species = label_encoder.inverse_transform([y_test.iloc[i]])[0]\n",
    "    predicted_species = label_encoder.inverse_transform([y_pred[i]])[0]\n",
    "    print(f\"Sample {i+1}: Actual: {actual_species}, Predicted: {predicted_species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Model evaluation involves calculating various metrics and visualizing the results. We use accuracy, precision, recall, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model\n",
    "print(\"\\nStep 6: Model Evaluation...\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Understanding which features are most important for the model's decisions helps in interpreting the results and potentially improving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Feature Importance\n",
    "print(\"\\nStep 7: Feature Importance Analysis...\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': dt_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance in Decision Tree Classifier')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Predictions\n",
    "\n",
    "We demonstrate how to use the trained model to make predictions on new, unseen data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Example predictions on new data\n",
    "print(\"\\nStep 8: Example of making predictions on new data...\")\n",
    "# Create some example measurements\n",
    "new_samples = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Likely Iris-setosa\n",
    "    [6.0, 2.7, 5.1, 1.6],  # Likely Iris-versicolor\n",
    "    [7.2, 3.6, 6.1, 2.5]   # Likely Iris-virginica\n",
    "])\n",
    "\n",
    "new_predictions = dt_classifier.predict(new_samples)\n",
    "new_probabilities = dt_classifier.predict_proba(new_samples)\n",
    "\n",
    "print(\"Predictions for new samples:\")\n",
    "for i, (pred, probs) in enumerate(zip(new_predictions, new_probabilities)):\n",
    "    species_name = label_encoder.inverse_transform([pred])[0]\n",
    "    print(f\"\\nSample {i+1}: {new_samples[i]}\")\n",
    "    print(f\"Predicted species: {species_name}\")\n",
    "    print(\"Probability distribution:\")\n",
    "    for j, prob in enumerate(probs):\n",
    "        species = label_encoder.inverse_transform([j])[0]\n",
    "        print(f\"  {species}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This section provides a final summary of the entire analysis, including key findings and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: Decision Tree Classifier\")\n",
    "print(f\"Dataset: Iris Species (150 samples, 3 classes)\")\n",
    "print(f\"Best performing feature: {feature_importance.iloc[0]['feature']}\")\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Model is {'GOOD' if accuracy > 0.9 else 'NEEDS IMPROVEMENT'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
